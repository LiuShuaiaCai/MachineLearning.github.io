<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>机器学习笔记</title>
  
  <subtitle>Deep Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ai.feifan.news/"/>
  <updated>2018-08-31T06:26:00.169Z</updated>
  <id>http://ai.feifan.news/</id>
  
  <author>
    <name>LiuShuaiaCai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>激活函数</title>
    <link href="http://ai.feifan.news/2018/08/30/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://ai.feifan.news/2018/08/30/激活函数/</id>
    <published>2018-08-30T06:06:08.000Z</published>
    <updated>2018-08-31T06:26:00.169Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、激活函数介绍"><a href="#1、激活函数介绍" class="headerlink" title="1、激活函数介绍"></a>1、激活函数介绍</h2><h3 id="1-1、什么是激活函数"><a href="#1-1、什么是激活函数" class="headerlink" title="1.1、什么是激活函数"></a>1.1、什么是激活函数</h3><p>如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。<br><img src="/images/active.png" alt="activate"><br><a id="more"></a></p><h3 id="1-2、激活函数的特点"><a href="#1-2、激活函数的特点" class="headerlink" title="1.2、激活函数的特点"></a>1.2、激活函数的特点</h3><p>非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。<br>可微： 当优化方法是基于梯度的时候，这个性质是必须的。<br>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。<br>_ ： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效。<br>输出值范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的学习率。</p><h3 id="1-3、为什么要用激活函数"><a href="#1-3、为什么要用激活函数" class="headerlink" title="1.3、为什么要用激活函数"></a>1.3、为什么要用激活函数</h3><p>如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。<br>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p><h2 id="2、sigmoid函数"><a href="#2、sigmoid函数" class="headerlink" title="2、sigmoid函数"></a>2、sigmoid函数</h2><p>公式：<script type="math/tex">\sigma(x) = \frac{1}{1 + e^{-x}}</script></p><p><img src="/images/sigmoid.png" alt="sigmoid"><br>在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间内，这点很有意思，可以联想到概率，但是严格意义上讲，不要当成概率。sigmod函数曾经是比较流行的，它可以想象成一个神经元的放电率，在中间斜率比较大的地方是神经元的敏感区，在两边斜率很平缓的地方是神经元的抑制区。<br>当然，流行也是曾经流行，这说明函数本身是有一定的缺陷的。</p><h3 id="2-1、sigmoid函数作用"><a href="#2-1、sigmoid函数作用" class="headerlink" title="2.1、sigmoid函数作用"></a>2.1、sigmoid函数作用</h3><p>sigmoid函数也叫 Logistic 函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。<br>在特征相差比较复杂或是相差不是特别大时效果比较好。</p><h3 id="2-2、sigmoid缺点"><a href="#2-2、sigmoid缺点" class="headerlink" title="2.2、sigmoid缺点"></a>2.2、sigmoid缺点</h3><p>1) 当输入稍微远离了坐标原点，函数的梯度就变得很小了，几乎为零。在神经网络反向传播的过程中，我们都是通过微分的链式法则来计算各个权重w的微分的。当反向传播经过了sigmod函数，这个链条上的微分就很小很小了，况且还可能经过很多个sigmod函数，最后会导致权重w对损失函数几乎没影响，这样不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散。<br>2) 函数输出不是以0为中心的，这样会使权重更新效率降低。对于这个缺陷，在斯坦福的课程里面有详细的解释。<br>3) Sigmoids函数收敛缓慢；sigmod函数要进行指数运算，这个对于计算机来说是比较慢的。</p><h3 id="2-3、sigmoid函数求导"><a href="#2-3、sigmoid函数求导" class="headerlink" title="2.3、sigmoid函数求导"></a>2.3、sigmoid函数求导</h3><p>下面解释为何会出现梯度消失：<br>反向传播算法中，要对激活函数求导，sigmoid 的导数表达式为：<script type="math/tex">\Phi'(x) =  \Phi(x)(1-\Phi(x))</script><br>求导推理：<script type="math/tex">\sigma'(x) = (\frac {1}{1 + e^{-x}})'</script><br>=&gt; <script type="math/tex">= \frac {e^{-x}}{(1 + e^{-x})^2}</script><br>=&gt; <script type="math/tex">= \frac {1+e^{-x}-1}{(1+e^{-x})^2}</script><br>=&gt; <script type="math/tex">= \frac {1}{1+e^{-x}}(1-\frac {1}{1+e^{-x}})</script><br><br><br>=&gt; <script type="math/tex">= f(x)(1-f(x))</script></p><p>求导图像如下：<br><img src="/images/sigmoid_re.png" alt="sigmoid"><br>由图可知，导数从 0 开始很快就又趋近于 0 了，易造成“梯度消失”现象</p><p>python代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="comment"># sigmoid函数导数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">re_sigmoid</span><span class="params">(y)</span>:</span></span><br><span class="line">    y_ = y * (<span class="number">1</span>-y)</span><br><span class="line">    <span class="keyword">return</span> y_</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid 显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_sigmoid</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = sigmoid(x)</span><br><span class="line">    y_ = re_sigmoid(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    ax.plot(y, y_)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">2</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-0.5</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">plot_sigmoid()</span><br></pre></td></tr></table></figure></p><h2 id="3、正切函数（tanh函数）"><a href="#3、正切函数（tanh函数）" class="headerlink" title="3、正切函数（tanh函数）"></a>3、正切函数（tanh函数）</h2><p>公式：<script type="math/tex">\tanh(x) = \frac {\sinh(x)}{\cosh(x)} = \frac {e^x - e^{-x}}{e^x + e^{-x}}</script><br><img src="/images/tanh.png" alt="tanh"><br>tanh是双曲正切函数，tanh函数和sigmod函数的曲线是比较相近的，咱们来比较一下看看。首先相同的是，这两个函数在输入很大或是很小的时候，输出都几乎平滑，梯度很小，不利于权重更新；不同的是输出区间，tanh的输出区间是在(-1,1)之间，而且整个函数是以0为中心的，这个特点比sigmod的好。</p><p>一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。不过这些也都不是一成不变的，具体使用什么激活函数，还是要根据具体的问题来具体分析，还是要靠调试的。</p><p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tanh函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x=<span class="number">0</span>, bool=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> bool==<span class="keyword">False</span>:</span><br><span class="line">        y = (np.exp(x) - <span class="number">1</span>/np.exp(x)) / (np.exp(x) + <span class="number">1</span>/np.exp(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = np.tanh((<span class="number">0</span>, np.pi*<span class="number">1j</span>, np.pi*<span class="number">1j</span>/<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># tanh函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tanh</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = tanh(x)</span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-1.0</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">plot_tanh()</span><br></pre></td></tr></table></figure></p><h2 id="4、线性整流函数（ReLU函数）"><a href="#4、线性整流函数（ReLU函数）" class="headerlink" title="4、线性整流函数（ReLU函数）"></a>4、线性整流函数（ReLU函数）</h2><h3 id="4-1、定义（来自wiki）"><a href="#4-1、定义（来自wiki）" class="headerlink" title="4.1、定义（来自wiki）"></a>4.1、定义（来自<a href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">wiki</a>）</h3><p>线性整流函数（Rectified Linear Unit, ReLU）,又称修正线性单元, 是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。<br>通常意义下，线性整流函数指代数学中的斜坡函数，即</p><script type="math/tex; mode=display">f(x)=max(0,x)</script><p>而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换 <script type="math/tex">\mathbf {w} ^{T}\mathbf {x} +b</script>之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量 x，使用线性整流激活函数的神经元会输出</p><script type="math/tex; mode=display">\max(0,\mathbf {w} ^{T}\mathbf {x} +b)</script><p>至下一层神经元或作为整个神经网络的输出（取决现神经元在网络结构中所处位置）。<br><img src="/images/relu.png" alt="relu"></p><h3 id="4-2、优、缺点"><a href="#4-2、优、缺点" class="headerlink" title="4.2、优、缺点"></a>4.2、优、缺点</h3><p>优点：<br>1) 在输入为正数的时候，不存在梯度饱和问题，相比之下，逻辑函数在输入为0时达到 <script type="math/tex">\frac {1}{2}</script>，即已经是半饱和的稳定状态。<br>2) 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）<br>3）更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题</p><p>缺点：<br>1) 当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。<br>2) 我们发现ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</p><h3 id="4-3、ReLU的几种变形"><a href="#4-3、ReLU的几种变形" class="headerlink" title="4.3、ReLU的几种变形"></a>4.3、ReLU的几种变形</h3><p>4.3.1、带泄露线性整流（Leaky ReLU）<br>在输入值 x 为负的时候，带泄露线性整流函数（Leaky ReLU）的梯度为一个常数 <script type="math/tex">\lambda \in (0,1)</script>，而不是0。在输入值为正的时候，带泄露线性整流函数和普通斜坡函数保持一致。</p><script type="math/tex; mode=display">f(x)={\begin{cases}x&{\mbox{if }}x>0\\\lambda x&{\mbox{if }}x\leq 0\end{cases}}</script><p>4.3.2、参数线性整流（Parametric ReLU）<br>在深度学习中，如果设定 <script type="math/tex">\lambda</script>  为一个可通过<a href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">反向传播算法</a>（Backpropagation）学习的变量，那么带泄露线性整流又被称为参数线性整流（Parametric ReLU）</p><p>4.3.3、带泄露随机线性整流（Randomized Leaky ReLU, RReLU）<br>带泄露随机线性整流（Randomized Leaky ReLU, RReLU）最早是在Kaggle全美数据科学大赛（NDSB）中被首先提出并使用的。相比于普通带泄露线性整流函数，带泄露随机线性整流在负输入值段的函数梯度 <script type="math/tex">\lambda</script>  是一个取自连续性均匀分布 <script type="math/tex">U(l,u)</script>概率模型的随机变量，即</p><script type="math/tex; mode=display">f(x)={\begin{cases}x&{\mbox{if }}x>0\\\lambda x&{\mbox{if }}x\leq 0\end{cases}}</script><p>其中 <script type="math/tex">\lambda \sim U(l,u),l<u</script> 且 <script type="math/tex">l,u\in [0,1)</script>。</p><p>ReLU函数的python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReLU函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        y = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># ReLU函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_relu</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</span><br><span class="line">    y = list(map(relu,x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line"></span><br><span class="line">plot_relu()</span><br></pre></td></tr></table></figure></p><h2 id="5、softmax函数（wiki）"><a href="#5、softmax函数（wiki）" class="headerlink" title="5、softmax函数（wiki）"></a>5、softmax函数（<a href="https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">wiki</a>）</h2><p>在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量  <script type="math/tex">\mathbf {z}</script> “压缩”到另一个K维实向量  <script type="math/tex">\sigma (\mathbf {z} )</script> 中，使得每一个元素的范围都在 (0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：</p><script type="math/tex; mode=display">\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} j \in (1,k)</script><p>如图所示：<br><img src="/images/softmax.png" alt="softmax"></p><p>Softmax函数实际上是有限项离散概率分布的梯度对数归一化。因此，Softmax函数在包括 多项逻辑回归[1]:206–209 ，多项线性判别分析，朴素贝叶斯分类器和人工神经网络等的多种基于概率的多分类问题方法中都有着广泛应用。[2] 特别地，在多项逻辑回归和线性判别分析中，函数的输入是从K个不同的线性函数得到的结果，而样本向量 x 属于第 j 个分类的概率为：</p><script type="math/tex; mode=display">P(y=j|\mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}</script><p>这可以被视作K个线性函数 <script type="math/tex">\mathbf {x} \mapsto \mathbf {x} ^{\mathsf {T}}\mathbf {w} _{1},\ldots ,\mathbf {x} \mapsto \mathbf {x} ^{\mathsf {T}}\mathbf {w} _{K}</script> Softmax函数的复合（ <script type="math/tex">{\displaystyle \mathbf {x} ^{\mathsf {T}}\mathbf {w} } {\displaystyle \mathbf {x} }  {\displaystyle \mathbf {w} }</script> ）。</p><p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = np.exp(x)/sum(np.exp(x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_softmax</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">    y = softmax(x)</span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-6</span>, <span class="number">6.1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-0.01</span>, <span class="number">0.12</span>, <span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">plot_softmax()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、激活函数介绍&quot;&gt;&lt;a href=&quot;#1、激活函数介绍&quot; class=&quot;headerlink&quot; title=&quot;1、激活函数介绍&quot;&gt;&lt;/a&gt;1、激活函数介绍&lt;/h2&gt;&lt;h3 id=&quot;1-1、什么是激活函数&quot;&gt;&lt;a href=&quot;#1-1、什么是激活函数&quot; class=&quot;headerlink&quot; title=&quot;1.1、什么是激活函数&quot;&gt;&lt;/a&gt;1.1、什么是激活函数&lt;/h3&gt;&lt;p&gt;如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。&lt;br&gt;&lt;img src=&quot;/images/active.png&quot; alt=&quot;activate&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>word2vec原理分析笔记</title>
    <link href="http://ai.feifan.news/2018/08/30/word2vec/"/>
    <id>http://ai.feifan.news/2018/08/30/word2vec/</id>
    <published>2018-08-30T03:46:09.000Z</published>
    <updated>2018-08-30T06:13:43.534Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、word2vec介绍"><a href="#1、word2vec介绍" class="headerlink" title="1、word2vec介绍"></a>1、word2vec介绍</h2><p>word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector (词向量)的工具包。<br>特点：简单、高效<br>作用：把文本转化为词向量</p><h2 id="2、预备知识"><a href="#2、预备知识" class="headerlink" title="2、预备知识"></a>2、预备知识</h2><p>word2vec 中用到的一些重要知识点：sigmoid函数、Beyes公式、Huffman编码<br><a id="more"></a></p><h4 id="2-1、sigmoid函数"><a href="#2-1、sigmoid函数" class="headerlink" title="2.1、sigmoid函数"></a>2.1、sigmoid函数</h4><p>sigmoid 函数是神经网络中常用的激活函数之一，其定义为：</p><script type="math/tex; mode=display">\overline{X} = \frac{\sum_{i=1}^{n}X_i}{n}</script><p><code>$x^{y^z} = (1+e^x)^{-2xy^w}$</code></p><script type="math/tex; mode=display">\require{enclose}\begin{array}{}\enclose{horizontalstrike}{x+y}\\\enclose{horizontalstrike}{x*y}\\\end{array}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、word2vec介绍&quot;&gt;&lt;a href=&quot;#1、word2vec介绍&quot; class=&quot;headerlink&quot; title=&quot;1、word2vec介绍&quot;&gt;&lt;/a&gt;1、word2vec介绍&lt;/h2&gt;&lt;p&gt;word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector (词向量)的工具包。&lt;br&gt;特点：简单、高效&lt;br&gt;作用：把文本转化为词向量&lt;/p&gt;
&lt;h2 id=&quot;2、预备知识&quot;&gt;&lt;a href=&quot;#2、预备知识&quot; class=&quot;headerlink&quot; title=&quot;2、预备知识&quot;&gt;&lt;/a&gt;2、预备知识&lt;/h2&gt;&lt;p&gt;word2vec 中用到的一些重要知识点：sigmoid函数、Beyes公式、Huffman编码&lt;br&gt;
    
    </summary>
    
    
      <category term="Word2Vec" scheme="http://ai.feifan.news/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>哈夫曼编码与哈夫曼树</title>
    <link href="http://ai.feifan.news/2018/08/30/hmm-1/"/>
    <id>http://ai.feifan.news/2018/08/30/hmm-1/</id>
    <published>2018-08-30T01:45:06.000Z</published>
    <updated>2018-08-30T06:14:07.820Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、HMM原理"><a href="#1、HMM原理" class="headerlink" title="1、HMM原理"></a>1、HMM原理</h2><p>ab是apachebench的缩写<br>ab命令会创建多个并发访问线程，对同一个URL访问来测试apache的负载压力。测试目标是URL。、Lighthttp、Tomcat、IIS等其它服务器</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、HMM原理&quot;&gt;&lt;a href=&quot;#1、HMM原理&quot; class=&quot;headerlink&quot; title=&quot;1、HMM原理&quot;&gt;&lt;/a&gt;1、HMM原理&lt;/h2&gt;&lt;p&gt;ab是apachebench的缩写&lt;br&gt;ab命令会创建多个并发访问线程，对同一个URL访问来测试apache的负载压力。测试目标是URL。、Lighthttp、Tomcat、IIS等其它服务器&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://ai.feifan.news/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="HMM" scheme="http://ai.feifan.news/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>在 hexo 中支持 Mathjax</title>
    <link href="http://ai.feifan.news/2018/08/30/mathjax/"/>
    <id>http://ai.feifan.news/2018/08/30/mathjax/</id>
    <published>2018-08-29T17:57:54.000Z</published>
    <updated>2018-08-30T03:59:06.580Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步-使用Kramed代替-Marked"><a href="#第一步-使用Kramed代替-Marked" class="headerlink" title="第一步 使用Kramed代替 Marked"></a>第一步 使用Kramed代替 Marked</h2><p><code>hexo</code> 默认的渲染引擎是 marked，但是 <code>marked</code> 不支持 <code>mathjax</code>。 <code>kramed</code> 是在 <code>marked</code> 的基础上进行修改。我们在工程目录下执行以下命令来安装 <code>kramed</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p><p>然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">    return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="第二步-停止使用-hexo-math"><a href="#第二步-停止使用-hexo-math" class="headerlink" title="第二步: 停止使用 hexo-math"></a>第二步: 停止使用 hexo-math</h2><p>首先，如果你已经安装 hexo-math, 请卸载它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure></p><p>然后安装 hexo-renderer-mathjax 包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure></p><h2 id="第三步-更新-Mathjax-的-CDN-链接"><a href="#第三步-更新-Mathjax-的-CDN-链接" class="headerlink" title="第三步: 更新 Mathjax 的 CDN 链接"></a>第三步: 更新 Mathjax 的 CDN 链接</h2><p>首先，打开/node_modules/hexo-renderer-mathjax/mathjax.html<br>然后，把<code>&lt;script&gt;</code>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="第四步-更改默认转义规则"><a href="#第四步-更改默认转义规则" class="headerlink" title="第四步: 更改默认转义规则"></a>第四步: 更改默认转义规则</h2><p>因为 hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 <em>, 所以我们需要对默认的规则进行修改.<br>首先， 打开 node_modules\kramed\lib\rules\inline.js<br>把<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure></em></p><p>更改为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([`*\[\]()# +\-.!_&gt;])/,</span><br></pre></td></tr></table></figure></p><p>把<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><h2 id="第五步-开启mathjax"><a href="#第五步-开启mathjax" class="headerlink" title="第五步: 开启mathjax"></a>第五步: 开启mathjax</h2><p>在主题 <code>_config.yml</code> 中开启 <code>Mathjax</code>， 找到 <code>mathjax</code> 字段添加如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></p><p>这一步可选，在博客中开启 Mathjax，， 添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Testing Mathjax with Hexo</span><br><span class="line">category: Uncategorized</span><br><span class="line">date: 2017/05/03</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>重新启动hexo（先clean再generate）<br>通过以上步骤，我们就可以在 hexo 中使用 Mathjax 来书写数学公式。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;第一步-使用Kramed代替-Marked&quot;&gt;&lt;a href=&quot;#第一步-使用Kramed代替-Marked&quot; class=&quot;headerlink&quot; title=&quot;第一步 使用Kramed代替 Marked&quot;&gt;&lt;/a&gt;第一步 使用Kramed代替 Marked&lt;/h2&gt;&lt;p&gt;&lt;code&gt;hexo&lt;/code&gt; 默认的渲染引擎是 marked，但是 &lt;code&gt;marked&lt;/code&gt; 不支持 &lt;code&gt;mathjax&lt;/code&gt;。 &lt;code&gt;kramed&lt;/code&gt; 是在 &lt;code&gt;marked&lt;/code&gt; 的基础上进行修改。我们在工程目录下执行以下命令来安装 &lt;code&gt;kramed&lt;/code&gt;.&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm uninstall hexo-renderer-marked --save&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-renderer-kramed --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
