<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>机器学习笔记</title>
  
  <subtitle>Deep Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ai.feifan.news/"/>
  <updated>2018-08-30T10:32:17.403Z</updated>
  <id>http://ai.feifan.news/</id>
  
  <author>
    <name>LiuShuaiaCai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>激活函数</title>
    <link href="http://ai.feifan.news/2018/08/30/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <id>http://ai.feifan.news/2018/08/30/激活函数/</id>
    <published>2018-08-30T06:06:08.000Z</published>
    <updated>2018-08-30T10:32:17.403Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、激活函数-Activation-Function"><a href="#1、激活函数-Activation-Function" class="headerlink" title="1、激活函数(Activation Function)"></a>1、激活函数(Activation Function)</h2><p>算法设计</p><h4 id="1-1、什么是激活函数"><a href="#1-1、什么是激活函数" class="headerlink" title="1.1、什么是激活函数"></a>1.1、什么是激活函数</h4><p>如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。<br><img src="/images/active.png" alt="activate"></p><h4 id="1-2、激活函数的特点"><a href="#1-2、激活函数的特点" class="headerlink" title="1.2、激活函数的特点"></a>1.2、激活函数的特点</h4><p>非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。<br>可微： 当优化方法是基于梯度的时候，这个性质是必须的。<br>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。<br>_ ： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效。<br>输出值范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的学习率。</p><h4 id="1-3、为什么要用激活函数"><a href="#1-3、为什么要用激活函数" class="headerlink" title="1.3、为什么要用激活函数"></a>1.3、为什么要用激活函数</h4><p>如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。<br>如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。</p><h2 id="2、sigmoid-函数"><a href="#2、sigmoid-函数" class="headerlink" title="2、sigmoid 函数"></a>2、sigmoid 函数</h2><p>公式：<script type="math/tex">\sigma(x) = \frac{1}{1 + e^{-x}}</script></p><p><img src="/images/sigmoid.png" alt="sigmoid"><br><a id="more"></a><br>在sigmod函数中我们可以看到，其输出是在(0,1)这个开区间内，这点很有意思，可以联想到概率，但是严格意义上讲，不要当成概率。sigmod函数曾经是比较流行的，它可以想象成一个神经元的放电率，在中间斜率比较大的地方是神经元的敏感区，在两边斜率很平缓的地方是神经元的抑制区。</p><p>当然，流行也是曾经流行，这说明函数本身是有一定的缺陷的。</p><p>1) 当输入稍微远离了坐标原点，函数的梯度就变得很小了，几乎为零。在神经网络反向传播的过程中，我们都是通过微分的链式法则来计算各个权重w的微分的。当反向传播经过了sigmod函数，这个链条上的微分就很小很小了，况且还可能经过很多个sigmod函数，最后会导致权重w对损失函数几乎没影响，这样不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散。</p><p>2) 函数输出不是以0为中心的，这样会使权重更新效率降低。对于这个缺陷，在斯坦福的课程里面有详细的解释。</p><p>3) sigmod函数要进行指数运算，这个对于计算机来说是比较慢的。<br>python代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid 显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_sigmoid</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = sigmoid(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">2</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-0.5</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">plot_sigmoid()</span><br></pre></td></tr></table></figure></p><h2 id="3、tanh函数"><a href="#3、tanh函数" class="headerlink" title="3、tanh函数"></a>3、tanh函数</h2><p>公式：<script type="math/tex">\tanh(x) = \frac {\sinh(x)}{\cosh(x)} = \frac {e^x - e^{-x}}{e^x + e^{-x}}</script><br><img src="/images/tanh.png" alt="tanh"><br>tanh是双曲正切函数，tanh函数和sigmod函数的曲线是比较相近的，咱们来比较一下看看。首先相同的是，这两个函数在输入很大或是很小的时候，输出都几乎平滑，梯度很小，不利于权重更新；不同的是输出区间，tanh的输出区间是在(-1,1)之间，而且整个函数是以0为中心的，这个特点比sigmod的好。</p><p>一般二分类问题中，隐藏层用tanh函数，输出层用sigmod函数。不过这些也都不是一成不变的，具体使用什么激活函数，还是要根据具体的问题来具体分析，还是要靠调试的。</p><p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tanh函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x=<span class="number">0</span>, bool=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> bool==<span class="keyword">False</span>:</span><br><span class="line">        y = (np.exp(x) - <span class="number">1</span>/np.exp(x)) / (np.exp(x) + <span class="number">1</span>/np.exp(x))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = np.tanh((<span class="number">0</span>, np.pi*<span class="number">1j</span>, np.pi*<span class="number">1j</span>/<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line"><span class="comment"># tanh函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_tanh</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-8</span>, <span class="number">8</span>, <span class="number">0.2</span>)</span><br><span class="line">    y = tanh(x)</span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line">    <span class="comment"># 设置边距</span></span><br><span class="line">    ax.set_xticks(np.arange(<span class="number">-5</span>, <span class="number">5.1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.set_yticks(np.arange(<span class="number">-1.0</span>, <span class="number">1.1</span>, <span class="number">0.5</span>))</span><br><span class="line">plot_tanh()</span><br></pre></td></tr></table></figure></p><h2 id="4、ReLU-函数"><a href="#4、ReLU-函数" class="headerlink" title="4、ReLU 函数"></a>4、ReLU 函数</h2><p>公式：<script type="math/tex">f(x) = max(0,x)</script><br><img src="/images/relu.png" alt="relu"><br>ReLU(Rectified Linear Unit)函数是目前比较火的一个激活函数，相比于sigmod函数和tanh函数，它有以下几个优点：</p><p>1) 在输入为正数的时候，不存在梯度饱和问题。</p><p>2) 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）</p><p>当然，缺点也是有的：</p><p>1) 当输入是负数的时候，ReLU是完全不被激活的，这就表明一旦输入到了负数，ReLU就会死掉。这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完全到0，这个和sigmod函数、tanh函数有一样的问题。</p><p>2) 我们发现ReLU函数的输出要么是0，要么是正数，这也就是说，ReLU函数也不是以0为中心的函数。</p><p>python 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReLU函数公式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        y = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = x</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># ReLU函数显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_relu</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</span><br><span class="line">    y = list(map(relu,x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置坐标原点</span></span><br><span class="line">    fig, ax = plt.subplots()</span><br><span class="line">    <span class="comment"># 隐藏上、右边</span></span><br><span class="line">    ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">    <span class="comment"># 设置左下边为轴</span></span><br><span class="line">    ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">    ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">    ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line">    ax.plot(x, y)</span><br><span class="line"></span><br><span class="line">plot_relu()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、激活函数-Activation-Function&quot;&gt;&lt;a href=&quot;#1、激活函数-Activation-Function&quot; class=&quot;headerlink&quot; title=&quot;1、激活函数(Activation Function)&quot;&gt;&lt;/a&gt;1、激活函数(Activation Function)&lt;/h2&gt;&lt;p&gt;算法设计&lt;/p&gt;
&lt;h4 id=&quot;1-1、什么是激活函数&quot;&gt;&lt;a href=&quot;#1-1、什么是激活函数&quot; class=&quot;headerlink&quot; title=&quot;1.1、什么是激活函数&quot;&gt;&lt;/a&gt;1.1、什么是激活函数&lt;/h4&gt;&lt;p&gt;如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。&lt;br&gt;&lt;img src=&quot;/images/active.png&quot; alt=&quot;activate&quot;&gt;&lt;/p&gt;
&lt;h4 id=&quot;1-2、激活函数的特点&quot;&gt;&lt;a href=&quot;#1-2、激活函数的特点&quot; class=&quot;headerlink&quot; title=&quot;1.2、激活函数的特点&quot;&gt;&lt;/a&gt;1.2、激活函数的特点&lt;/h4&gt;&lt;p&gt;非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。&lt;br&gt;可微： 当优化方法是基于梯度的时候，这个性质是必须的。&lt;br&gt;单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。&lt;br&gt;_ ： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效。&lt;br&gt;输出值范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的学习率。&lt;/p&gt;
&lt;h4 id=&quot;1-3、为什么要用激活函数&quot;&gt;&lt;a href=&quot;#1-3、为什么要用激活函数&quot; class=&quot;headerlink&quot; title=&quot;1.3、为什么要用激活函数&quot;&gt;&lt;/a&gt;1.3、为什么要用激活函数&lt;/h4&gt;&lt;p&gt;如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。&lt;br&gt;如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。&lt;/p&gt;
&lt;h2 id=&quot;2、sigmoid-函数&quot;&gt;&lt;a href=&quot;#2、sigmoid-函数&quot; class=&quot;headerlink&quot; title=&quot;2、sigmoid 函数&quot;&gt;&lt;/a&gt;2、sigmoid 函数&lt;/h2&gt;&lt;p&gt;公式：&lt;script type=&quot;math/tex&quot;&gt;\sigma(x) = \frac{1}{1 + e^{-x}}&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/sigmoid.png&quot; alt=&quot;sigmoid&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>word2vec原理分析笔记</title>
    <link href="http://ai.feifan.news/2018/08/30/word2vec/"/>
    <id>http://ai.feifan.news/2018/08/30/word2vec/</id>
    <published>2018-08-30T03:46:09.000Z</published>
    <updated>2018-08-30T06:13:43.534Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、word2vec介绍"><a href="#1、word2vec介绍" class="headerlink" title="1、word2vec介绍"></a>1、word2vec介绍</h2><p>word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector (词向量)的工具包。<br>特点：简单、高效<br>作用：把文本转化为词向量</p><h2 id="2、预备知识"><a href="#2、预备知识" class="headerlink" title="2、预备知识"></a>2、预备知识</h2><p>word2vec 中用到的一些重要知识点：sigmoid函数、Beyes公式、Huffman编码<br><a id="more"></a></p><h4 id="2-1、sigmoid函数"><a href="#2-1、sigmoid函数" class="headerlink" title="2.1、sigmoid函数"></a>2.1、sigmoid函数</h4><p>sigmoid 函数是神经网络中常用的激活函数之一，其定义为：</p><script type="math/tex; mode=display">\overline{X} = \frac{\sum_{i=1}^{n}X_i}{n}</script><p><code>$x^{y^z} = (1+e^x)^{-2xy^w}$</code></p><script type="math/tex; mode=display">\require{enclose}\begin{array}{}\enclose{horizontalstrike}{x+y}\\\enclose{horizontalstrike}{x*y}\\\end{array}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、word2vec介绍&quot;&gt;&lt;a href=&quot;#1、word2vec介绍&quot; class=&quot;headerlink&quot; title=&quot;1、word2vec介绍&quot;&gt;&lt;/a&gt;1、word2vec介绍&lt;/h2&gt;&lt;p&gt;word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector (词向量)的工具包。&lt;br&gt;特点：简单、高效&lt;br&gt;作用：把文本转化为词向量&lt;/p&gt;
&lt;h2 id=&quot;2、预备知识&quot;&gt;&lt;a href=&quot;#2、预备知识&quot; class=&quot;headerlink&quot; title=&quot;2、预备知识&quot;&gt;&lt;/a&gt;2、预备知识&lt;/h2&gt;&lt;p&gt;word2vec 中用到的一些重要知识点：sigmoid函数、Beyes公式、Huffman编码&lt;br&gt;
    
    </summary>
    
    
      <category term="Word2Vec" scheme="http://ai.feifan.news/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>哈夫曼编码与哈夫曼树</title>
    <link href="http://ai.feifan.news/2018/08/30/hmm-1/"/>
    <id>http://ai.feifan.news/2018/08/30/hmm-1/</id>
    <published>2018-08-30T01:45:06.000Z</published>
    <updated>2018-08-30T06:14:07.820Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、HMM原理"><a href="#1、HMM原理" class="headerlink" title="1、HMM原理"></a>1、HMM原理</h2><p>ab是apachebench的缩写<br>ab命令会创建多个并发访问线程，对同一个URL访问来测试apache的负载压力。测试目标是URL。、Lighthttp、Tomcat、IIS等其它服务器</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、HMM原理&quot;&gt;&lt;a href=&quot;#1、HMM原理&quot; class=&quot;headerlink&quot; title=&quot;1、HMM原理&quot;&gt;&lt;/a&gt;1、HMM原理&lt;/h2&gt;&lt;p&gt;ab是apachebench的缩写&lt;br&gt;ab命令会创建多个并发访问线程，对同一个URL访问来测试apache的负载压力。测试目标是URL。、Lighthttp、Tomcat、IIS等其它服务器&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="http://ai.feifan.news/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="HMM" scheme="http://ai.feifan.news/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>在 hexo 中支持 Mathjax</title>
    <link href="http://ai.feifan.news/2018/08/30/mathjax/"/>
    <id>http://ai.feifan.news/2018/08/30/mathjax/</id>
    <published>2018-08-29T17:57:54.000Z</published>
    <updated>2018-08-30T03:59:06.580Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一步-使用Kramed代替-Marked"><a href="#第一步-使用Kramed代替-Marked" class="headerlink" title="第一步 使用Kramed代替 Marked"></a>第一步 使用Kramed代替 Marked</h2><p><code>hexo</code> 默认的渲染引擎是 marked，但是 <code>marked</code> 不支持 <code>mathjax</code>。 <code>kramed</code> 是在 <code>marked</code> 的基础上进行修改。我们在工程目录下执行以下命令来安装 <code>kramed</code>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p><p>然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">    return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="第二步-停止使用-hexo-math"><a href="#第二步-停止使用-hexo-math" class="headerlink" title="第二步: 停止使用 hexo-math"></a>第二步: 停止使用 hexo-math</h2><p>首先，如果你已经安装 hexo-math, 请卸载它：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br></pre></td></tr></table></figure></p><p>然后安装 hexo-renderer-mathjax 包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure></p><h2 id="第三步-更新-Mathjax-的-CDN-链接"><a href="#第三步-更新-Mathjax-的-CDN-链接" class="headerlink" title="第三步: 更新 Mathjax 的 CDN 链接"></a>第三步: 更新 Mathjax 的 CDN 链接</h2><p>首先，打开/node_modules/hexo-renderer-mathjax/mathjax.html<br>然后，把<code>&lt;script&gt;</code>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure></p><h2 id="第四步-更改默认转义规则"><a href="#第四步-更改默认转义规则" class="headerlink" title="第四步: 更改默认转义规则"></a>第四步: 更改默认转义规则</h2><p>因为 hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 <em>, 所以我们需要对默认的规则进行修改.<br>首先， 打开 node_modules\kramed\lib\rules\inline.js<br>把<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure></em></p><p>更改为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">escape: /^\\([`*\[\]()# +\-.!_&gt;])/,</span><br></pre></td></tr></table></figure></p><p>把<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>更改为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><h2 id="第五步-开启mathjax"><a href="#第五步-开启mathjax" class="headerlink" title="第五步: 开启mathjax"></a>第五步: 开启mathjax</h2><p>在主题 <code>_config.yml</code> 中开启 <code>Mathjax</code>， 找到 <code>mathjax</code> 字段添加如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></p><p>这一步可选，在博客中开启 Mathjax，， 添加以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Testing Mathjax with Hexo</span><br><span class="line">category: Uncategorized</span><br><span class="line">date: 2017/05/03</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>重新启动hexo（先clean再generate）<br>通过以上步骤，我们就可以在 hexo 中使用 Mathjax 来书写数学公式。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;第一步-使用Kramed代替-Marked&quot;&gt;&lt;a href=&quot;#第一步-使用Kramed代替-Marked&quot; class=&quot;headerlink&quot; title=&quot;第一步 使用Kramed代替 Marked&quot;&gt;&lt;/a&gt;第一步 使用Kramed代替 Marked&lt;/h2&gt;&lt;p&gt;&lt;code&gt;hexo&lt;/code&gt; 默认的渲染引擎是 marked，但是 &lt;code&gt;marked&lt;/code&gt; 不支持 &lt;code&gt;mathjax&lt;/code&gt;。 &lt;code&gt;kramed&lt;/code&gt; 是在 &lt;code&gt;marked&lt;/code&gt; 的基础上进行修改。我们在工程目录下执行以下命令来安装 &lt;code&gt;kramed&lt;/code&gt;.&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;npm uninstall hexo-renderer-marked --save&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;npm install hexo-renderer-kramed --save&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;然后，更改/node_modules/hexo-renderer-kramed/lib/renderer.js，更改：&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
